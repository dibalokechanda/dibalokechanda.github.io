---
title: Conditional Independance, d-Separation, Markov Property and Markov Blanket
date: 2023-12-15 12:00:00 -500
categories: [fundamentals]
tags: [distributions, probabilistic_graphical_model]
math: true
toc: true
---

This post is largely inspired by the following lectures:
 

- [Probabilistic Graphical Models Lecture Series by Daphne Koller](https://www.coursera.org/specializations/probabilistic-graphical-models)

-  [MLSS 2013 TÃ¼bingen Lectures by Christopher Bishop](https://youtu.be/c0AWH5UFyOk)  

Also, the topics I mention here are pretty abstract and I do not provide any examples to make it more intuitive. I encourage interested readers to follow the [lecture series from UC Berkeley (by Prof. Dan Klein)](https://youtu.be/iaY3isLZUGs) for some nice examples.

## Conditional Independence

Before going into this topic, it is better to review the concept of independence between random variables. Suppose $a$ and $b$ are two random variables and they respectively follow the probability distribution $p(a)$ and $p(b)$. We can say $a$ and $b$ are independent if,

$$
p(a,b)=p(a) p(b)
$$

where, $p(a,b)$  is the joint distribution of $a$ and $b$. 

This can be generalized to more than two random variables. 


To formulate the framework for conditional independence, we need three random variables $a$, $b$, and $c$. Suppose they follow the probability distribution $p(a)$, $p(b)$ and $p(c)$. We say random variables $a$ and $b$ are conditionally independent given $c$ if the following expression is true in general:


$$
p(a,b|c) = p(a|c) p(b|c)
$$



## Conditional Independence in Directed Graphical Model

First of all,  let's summarise the concept of a directed graphical model. In short, it presents the dependencies among random variables through a directed acyclic graph, where nodes correspond to variables and directed edges indicate direct influences between them. Now to understand the conditional independence in the context of PGM, we need to consider the three elemental structures.


![diag](https://i.ibb.co/thhXdhD/chrome-O7-XKst55-N5.png)


The names for these three elemental structures are "Causal Chain Structure", "Common Effect Structure" and "Common Cause Structure".


### Common Effect Structure

![diag2](https://i.ibb.co/6nLyZLV/chrome-TMOpz-Drd-K0.png)

Now if we consider the case when $c$ is not observed, we can break down the joint probability distribution as follows:

$$
p(a,b,c)=p(c)p(a|c)p(b|c)
$$


If we write down $p(a,b)$ by trying to integrate out $c$, we can easily see that $p(a,b)$ can not be factored in $p(a)$ and $p(b)$.

$$
p(a, b)=\sum_c p(a, b, c)=\sum_c p(c) p(a \mid c) p(b \mid c) \neq  p(a) p(b)
$$


![diag3](https://i.ibb.co/FmLp665/chrome-CTw-PMvvg-PC.png)

Now if we consider the case when $c$ is observed, we can prove with Bayes theorem that $a$ and $b$ are conditionally independent given $c$.  

$$
 p(a, b \mid c)=\frac{p(a, b, c)}{p(c)} = \frac{p(c) p(a \mid c) p(b \mid c)}{p(c)}  =p(a|c) p(b \mid c)
$$

### Causal Chain Structure

![diag4](https://i.ibb.co/0r9mSRP/chrome-1-ARX7-Pfgnn.png)

Now we can break down the joint distribution as follows:

$$
p(a,b,c) = p(a) p(c|a) p(b|c)
$$

If we proceed by applying Bayes theorem,


$$

p(b|a,c)= \frac{p(a,b,c)}{p(a,c)} = \frac{ p(a) p(c|a) p(b|c)}{p(a)p(c|a)}= p(b|c)

$$

We just proved $p(b|a,c)=p(b|c)$ which means conditioning on $a$ does not have any effect on $b$. In other words, $a$ and $b$ are conditionally independent given $c$.


"Evidence along a chain breaks the chain"