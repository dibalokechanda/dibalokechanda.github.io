---
title: Evidence Lower Bound
date: 2023-08-26 12:00:00 -500
categories: [fundamentals]
tags: [variational_inference, latent_variables]
math: true
toc: true
---


Evidence lower bound, commonly known as <b>ELBO</b> is one of the fundamental concepts to understand variational inference and also probabilistic decision theory. I am going to assume that readers are familiar with Bayes Theorem which is a prerequisite concept. If you are not I encourage you to go through some materials like this [video](https://youtu.be/j4yxsEQqPMI?list=PL05umP7R6ij0bo4UtMdzEJ6TiLOqj4ZCm). Also, go through my article about [Latent Variable Model](https://dibalokechanda.github.io/posts/latent-variable-model-blog/).

## Intractability of Distribution

The main reason to come up with the concept of ELBO (at least originally) is due to the intractability of computing distributions in higher dimensions. In my article about [latent variable models](https://dibalokechanda.github.io/posts/latent-variable-model-blog/) I mention the intractability of the following quantity in higher dimensions. 

$$
p(x)=\int_z p(x \mid z) p(z) d z
$$

Remember that computing this quantity was a sub-goal, the main goal was to compute the posterior shown in the following equation:

$$
p(z \mid x) = \frac{p(x\mid z) p(z)}{\int_z  p(x\mid z) \ p(z)}
$$

If we cannot compute $\int_z  p(x\mid z) \ p(z)$, then we cannot compute the posterior $p(z \mid x) $. Now rather than trying to compute the exact value for $p(z \mid x)$ we use variational inference to approximate it with another distribution. In this article, I do not want to go deep into variational inference. Hence, the following section gives a small informal introduction to variational inference.

## A Hand-Wavy Introduction to Variational Inference 

An informal definition of variational inference is that it is a way to approximate a complex distribution with a relatively simpler distribution through the process of optimization. But for optimization, we need a loss function that will tell us how "good" our approximation is. If our approximation is good, we should stop the optimization process. As we are trying to compare between distributions, the obvious loss function that comes to mind is KL Divergence. As KL Divergence quantifies the similarity between two distributions, it can be used as an indicator of how "good" our approximation is. But there is a certain issue with using KL Divergence which is mentioned in the next section. Before we move on to the next section, let's introduce some terminology and symbols. As I said, the surrogate distribution is a simpler class of distribution we need to explicitly mention the parameters. Assume the exact posterior distribution is $p_{\theta}(z \mid x)$ and the surrogate posterior is $q_{\phi}(z \mid x)$.
## The Issue with  using KL Divergence as a Loss Function

If we write down the KL Divergence formula between  $p_{\theta}(z \mid x)$ and  $q_{\phi}(z \mid x)$ we get the following:


$$
\begin{aligned}
D_{K L}\left(q_\phi(z \mid x) \| p_{\theta}(z \mid x)\right) & =\int_z q_\phi(z \mid x) \log \frac{q_\phi(z \mid x)}{p_{\theta}(z \mid x)} d z \\

\end{aligned}
$$

Well, calculating this quantity requires knowing the posterior $p_{\theta}(z \mid x)$. This creates a kind of "chicken and egg" problem. The work-around is getting an expression for <b>ELBO </b> which is shown in the next section. 


## Derivation

$$
\begin{aligned}
D_{K L}\left(q_\phi(z \mid x) \| p_{\theta}(z \mid x)\right) & =\int_z q_\phi(z \mid x) \log \frac{q_\phi(z \mid x)}{p_{\theta}(z \mid x)} d z \\
& =-\int_z q_\phi(z \mid x) \log \frac{p_{\theta}(z \mid x)}{q_\phi(z \mid x)} d z \\

\end{aligned}
$$

Now we can replace $p_{\theta}(z \mid x)$ with following quantity,

$$
p_{\theta}(z \mid x) = \frac{p_{\theta}(z,x)}{p_{\theta}(x)}

$$

After replacing we get the following:

$$
\begin{aligned}
D_{K L}\left(q_\phi(z \mid x) \| p_{\theta}(z \mid x)\right)

& =-\int_z q_\phi(z \mid x) \log \frac{p_{\theta}(z, x)}{q_\phi(z \mid x) p_{\theta}(x)} d z \\
& =-\left(\int_z q_\phi(z \mid x) \log \frac{p_{\theta}(z, x)}{q_\phi(z \mid x)} d z-\int_z q_\phi(z \mid x) \log p_{\theta}(x) d z\right) \\
& =-\int_z q_\phi(z \mid x) \log \frac{p_{\theta}(z, x)}{q_\phi(z \mid x)} d z+\log p_{\theta}(x) .
\end{aligned}
$$



## Usage of ELBO


# References 

[1] [Jiang, Y. (2021). ELBO — What & Why. [online] Yunfan’s Blog. Available at: https://yunfanj.com/blog/2021/01/11/ELBO.html [Accessed 28 Aug. 2023]](https://yunfanj.com/blog/2021/01/11/ELBO.html).

[2] [Sachdeva, K. (2021). Evidence Lower Bound (ELBO) - CLEARLY EXPLAINED! [online] www.youtube.com. Available at: https://www.youtube.com/watch?v=IXsA5Rpp25w [Accessed 26 Aug. 2023]](https://www.youtube.com/watch?v=IXsA5Rpp25w).